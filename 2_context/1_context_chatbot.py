import json  # JSON ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import boto3  # AWS SDK for Python
import streamlit as st  # ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ ìœ„í•œ Streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬

# AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
bedrock_runtime = boto3.client(service_name="bedrock-runtime", region_name="us-east-1")

# Streamlit ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì œëª© ì„¤ì •
st.title("Chatbot Ver.2 : ëŒ€í™” ë§¥ë½ ì´í•´ ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

if "token_usage" not in st.session_state:
    st.session_state.token_usage = {
        "input_tokens": 0,  # ëˆ„ì  ì…ë ¥ í† í° ìˆ˜
        "output_tokens": 0,  # ëˆ„ì  ì¶œë ¥ í† í° ìˆ˜
        "total_tokens": 0,  # ëˆ„ì  í†µí•© í† í° ìˆ˜
        "last_input_tokens": 0,  # ìµœê·¼ ì…ë ¥ í† í° ìˆ˜
        "last_output_tokens": 0,  # ìµœê·¼ ì¶œë ¥ í† í° ìˆ˜
        "last_total_tokens": 0,  # ìµœê·¼ í†µí•© í† í° ìˆ˜
    }


def get_response_from_bedrock(messages):
    """
    Bedrock ëª¨ë¸ì— ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ëŠ” í•¨ìˆ˜
    :param messages: ëŒ€í™” íˆìŠ¤í† ë¦¬ (role, content í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸)
    :return: ëª¨ë¸ì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    try:
        # Bedrock ëª¨ë¸ì— ìš”ì²­í•  body ìƒì„±
        body = json.dumps(
            {
                "anthropic_version": "bedrock-2023-05-31",  # ëª¨ë¸ ë²„ì „
                "max_tokens": 1000,  # ì‘ë‹µì˜ ìµœëŒ€ í† í° ìˆ˜
                "messages": messages,  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
            }
        )

        # Bedrock ëª¨ë¸ í˜¸ì¶œ
        response = bedrock_runtime.invoke_model(
            modelId=FILL_ME_IN,
            body=body,  # ìš”ì²­ ë³¸ë¬¸
        )
        response_body = json.loads(response.get("body").read())  # ì‘ë‹µ ë³¸ë¬¸ íŒŒì‹±

        ### AI ì‘ë‹µ ë°ì´í„°ì˜ í˜•ì‹ê³¼ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”. ###
        ### ì‘ë‹µ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ì•„ë˜ ì¶”ì¶œ ë°ì´í„°ë¥¼ ì ì ˆí•˜ê²Œ ë³€ìˆ˜ì— í• ë‹¹í•˜ì„¸ìš”.

        # ëª¨ë¸ ì¶œë ¥ ì¶”ì¶œ
        output_text = FILL_ME_IN

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
        input_tokens = FILL_ME_IN  # ì…ë ¥ í† í° ìˆ˜ ì¶”ì¶œ
        output_tokens = FILL_ME_IN  # ì¶œë ¥ í† í° ìˆ˜ ì¶”ì¶œ
        total_tokens = input_tokens + output_tokens  # ì…ì¶œë ¥ í† í° ìˆ˜ ê³„ì‚°

        # ì„¸ì…˜ ìƒíƒœì— í† í° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸

        ## ìµœì‹  í† í° ê¸°ë¡
        st.session_state.token_usage["last_input_tokens"] = input_tokens
        st.session_state.token_usage["last_output_tokens"] = output_tokens
        st.session_state.token_usage["last_total_tokens"] = total_tokens

        ## ëˆ„ì  í† í° ê¸°ë¡
        st.session_state.token_usage["input_tokens"] += input_tokens
        st.session_state.token_usage["output_tokens"] += output_tokens
        st.session_state.token_usage["total_tokens"] += total_tokens

        return output_text
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
        st.error(f"Bedrockê³¼ í†µì‹  ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        return "ì‘ë‹µ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ"


# í™”ë©´ ìƒë‹¨ì— st.metricìœ¼ë¡œ í† í° ì‚¬ìš©ëŸ‰ í‘œì‹œ
header, button = st.columns(2)
with header:
    st.subheader("í† í° ì‚¬ìš©ëŸ‰ í™•ì¸", divider="rainbow")
with button:
    if st.button("í† í° ì‚¬ìš©ëŸ‰ ìƒˆë¡œê³ ì¹¨"):
        st.rerun()

current, total = st.tabs(["ìµœì‹ ", "ëˆ„ì "])
with current:
    input_token, output_token, all_token = st.columns(3)
    with input_token:
        st.metric(
            label="ìµœê·¼ ì…ë ¥ í† í° ìˆ˜",
            value=st.session_state.token_usage["last_input_tokens"],
        )
    with output_token:
        st.metric(
            label="ìµœê·¼ ì¶œë ¥ í† í° ìˆ˜",
            value=st.session_state.token_usage["last_output_tokens"],
        )
    with all_token:
        st.metric(
            label="ìµœê·¼ ì…ì¶œë ¥ í† í° ìˆ˜",
            value=st.session_state.token_usage["last_total_tokens"],
        )
with total:
    total_input_token, total_output_token, total_all_token = st.columns(3)
    with total_input_token:
        st.metric(
            label="ëˆ„ì  ì…ë ¥ í† í° ìˆ˜",
            value=st.session_state.token_usage["input_tokens"],
        )
    with total_output_token:
        st.metric(
            label="ëˆ„ì  ì¶œë ¥ í† í° ìˆ˜",
            value=st.session_state.token_usage["output_tokens"],
        )
    with total_all_token:
        st.metric(
            label="ëˆ„ì  ì…ì¶œë ¥ í† í° ìˆ˜",
            value=st.session_state.token_usage["total_tokens"],
        )

st.divider()
st.subheader("ì±—ë´‡ ğŸ¤– ğŸ’¬", divider="rainbow")

# ì €ì¥ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):  # ë©”ì‹œì§€ ì—­í• ì— ë”°ë¥¸ ì±„íŒ… ë²„ë¸” ìƒì„±
        st.markdown(message["content"])  # ë©”ì‹œì§€ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë Œë”ë§

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("Message Bedrock..."):  # ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìŒ
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)  # ì…ë ¥ ë©”ì‹œì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë Œë”ë§

    # Bedrockì— ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        response = get_response_from_bedrock(
            st.session_state.messages
        )  # ì „ì²´ íˆìŠ¤í† ë¦¬ ì „ë‹¬
        st.markdown(response)  # ëª¨ë¸ ì‘ë‹µì„ í™”ë©´ì— í‘œì‹œ

    # ëª¨ë¸ì˜ ì‘ë‹µì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": response})
