import json
import streamlit as st
import pandas as pd
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter

st.set_page_config(page_title="RAG ë¬¸ì„œ ë¶„ì„ê¸°", page_icon="ğŸ“„", layout="wide")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ìŠ¤í”Œë¦¬í„° ì„¤ì •")
    chunk_size = st.slider("ì²­í¬ ì‚¬ì´ì¦ˆ", 100, 1000, 500, 50)
    chunk_overlap = st.slider("ì˜¤ë²„ë© í¬ê¸°", 0, 200, 100, 10)
    separator = st.text_input("êµ¬ë¶„ì", value="\n")

# ë©”ì¸ ë¡œì§
try:
    # ìŠ¤í”Œë¦¬í„° ì´ˆê¸°í™”
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator=separator,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        disallowed_special=(),
    )

    # PDF ë¡œë”© ë° ë¶„í• 
    pdf_loader = PyPDFLoader("./data/univ-data.pdf")
    pdf = pdf_loader.load()
    data = pdf_loader.load_and_split(text_splitter=splitter)

    # ë©”íŠ¸ë¦­ ì„¹ì…˜
    st.subheader("ìŠ¤í”Œë¦¬í„° ì„¤ì •", divider="rainbow")
    metrics = st.columns(3)
    metrics[0].metric("ì²­í¬ ì‚¬ì´ì¦ˆ", chunk_size, "í† í°", border=True)
    metrics[1].metric("ì˜¤ë²„ë© ë²”ìœ„", chunk_overlap, "í† í°", border=True)
    metrics[2].metric("ë‚˜ëˆ ì§„ ë°ì´í„° ìˆ˜", len(data), "ì²­í¬", border=True)

    # ë°ì´í„° í”„ë¦¬ë·° ì„¹ì…˜
    st.subheader("ë°ì´í„° í”„ë¦¬ë·°", divider="rainbow")
    preview_cols = st.columns(2)

    for i, col in enumerate(preview_cols):
        if i < len(data):
            with col:
                st.subheader(f"ì²­í¬ #{i+1}", divider="rainbow")
                st.text_area(
                    "ë‚´ìš©", value=data[i].page_content, height=300, disabled=True
                )
                st.json(data[i].metadata)

    # ë°ì´í„° ë¶„ì„ ì„¹ì…˜
    st.subheader("ë°ì´í„° ë¶„ì„", divider="rainbow")
    analysis_cols = st.columns(2)

    # ì²­í¬ ê¸¸ì´ ë¶„í¬
    with analysis_cols[0]:
        chunk_lengths = [len(chunk.page_content) for chunk in data]
        st.write("ì²­í¬ ê¸¸ì´ í†µê³„")
        st.write(
            {
                "ìµœì†Œ ê¸¸ì´": min(chunk_lengths),
                "ìµœëŒ€ ê¸¸ì´": max(chunk_lengths),
                "í‰ê·  ê¸¸ì´": sum(chunk_lengths) / len(chunk_lengths),
            }
        )

    # í˜ì´ì§€ë³„ ì²­í¬ ìˆ˜
    with analysis_cols[1]:
        page_chunks = {}
        for chunk in data:
            page = chunk.metadata.get("page", 0)
            page_chunks[page] = page_chunks.get(page, 0) + 1
        st.write("í˜ì´ì§€ë³„ ì²­í¬ ìˆ˜")
        st.write(page_chunks)

    # ì „ì²´ ë°ì´í„° í…Œì´ë¸” (ì ‘ì„ ìˆ˜ ìˆëŠ” ì„¹ì…˜)
    with st.expander("ì „ì²´ ë°ì´í„° ë³´ê¸°"):
        df = pd.DataFrame(
            [
                {
                    "ì²­í¬ ë²ˆí˜¸": i + 1,
                    "í˜ì´ì§€": chunk.metadata.get("page", 0),
                    "ë‚´ìš©": chunk.page_content,
                    "ê¸¸ì´": len(chunk.page_content),
                }
                for i, chunk in enumerate(data)
            ]
        )
        st.dataframe(df, use_container_width=True)

except Exception as e:
    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    st.stop()

# í‘¸í„°
st.markdown("---")
st.caption("RAG ë¬¸ì„œ ë¶„ì„ê¸° v1.0")
