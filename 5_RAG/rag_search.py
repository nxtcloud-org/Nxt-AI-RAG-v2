import boto3
import streamlit as st
from langchain_aws import ChatBedrock
from langchain_aws import BedrockEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
import chromadb

# ì‚¬ì´ë“œë°” ìë™ ìˆ¨ê¹€ ì„¤ì •
st.set_page_config(initial_sidebar_state="collapsed")
st.title("ğŸ” í•™ì‚¬ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
st.caption("RAG(Retrieval-Augmented Generation) ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰")


# AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def init_bedrock():
    bedrock_client = boto3.client("bedrock-runtime", region_name="us-east-1")
    bedrock = ChatBedrock(
        client=bedrock_client,
        model_id="anthropic.claude-3-haiku-20240307-v1:0",
        model_kwargs={"anthropic_version": "bedrock-2023-05-31"},
    )
    embeddings = BedrockEmbeddings(region_name="us-east-1")
    return bedrock, embeddings


bedrock, embeddings = init_bedrock()


# PDF ë¡œë“œ ë° ì²­í¬ ë¶„í• 
@st.cache_resource
def load_and_process_pdf():
    chroma_client = chromadb.PersistentClient(path="./vector_db")
    # íŒŒì¼ë¡œë“œ
    pdf_loader = PyMuPDFLoader("./data/univ-data.pdf")
    # ì²­í¬ ë¶„í• 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=500,
        chunk_overlap=50,
    )
    data = pdf_loader.load_and_split(text_splitter=splitter)
    # ë²¡í„° ìŠ¤í† ì–´ êµ¬ì„±
    vectorstore = Chroma.from_documents(
        documents=data,
        embedding=embeddings,
        persist_directory="./vector_db/",
        collection_name="university_docs",
    )
    return vectorstore


# ë©”ì¸ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤
try:
    vectorstore = load_and_process_pdf()
    st.success("ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    st.header("ğŸ“š í•™ì‚¬ ì •ë³´ ê²€ìƒ‰")

    search_query = st.text_input(
        "ê¶ê¸ˆí•œ ë‚´ìš©ì„ ìì—°ì–´ë¡œ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ì¡¸ì—…ìš”ê±´ì´ ë­ì•¼?",
        key="search_query",
    )

    if search_query:  # ê²€ìƒ‰ì–´ê°€ ì…ë ¥ëœ ê²½ìš°ì—ë§Œ ê²€ìƒ‰ ì‹¤í–‰
        results = vectorstore.similarity_search(search_query, k=3)

        # ì¤‘ë³µ ì œê±°
        seen_contents = set()
        unique_results = []
        for doc in results:
            content = doc.page_content.strip()
            if content not in seen_contents:
                seen_contents.add(content)
                unique_results.append(doc)

        st.write(f"ğŸ¯ ê²€ìƒ‰ ê²°ê³¼: {len(unique_results)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
        for i, doc in enumerate(unique_results, 1):
            with st.expander(f"ê²€ìƒ‰ ê²°ê³¼ #{i}"):
                st.markdown(f"**ë‚´ìš©:**\n{doc.page_content}")
                st.caption(f"ì¶œì²˜: {doc.metadata.get('page', 'N/A')}í˜ì´ì§€")

        # AI ë‹µë³€ ìƒì„±
        st.write("---")
        st.subheader("ğŸ¤– AI ì‘ë‹µ")

        with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            context = "\n".join([doc.page_content for doc in unique_results])

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒì€ í•™ì‚¬ ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ê³¼ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:
    
    ì§ˆë¬¸: {search_query}
    
    ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
    {context}
    
    ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
    ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ë§ê³ , í™•ì‹¤í•œ ì •ë³´ë§Œ ë‹µë³€ì— í¬í•¨í•´ì£¼ì„¸ìš”.
    """

            # AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
            response = bedrock.invoke(prompt)

            # ì‘ë‹µ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„° ë¶„ë¦¬í•˜ì—¬ í‘œì‹œ
            st.markdown("**ë‹µë³€ ë‚´ìš©:**")
            st.markdown(response.content)

            # ë©”íƒ€ë°ì´í„°ë¥¼ ì ‘ì„ ìˆ˜ ìˆëŠ” expanderë¡œ í‘œì‹œ
            with st.expander("ğŸ“Š ì‘ë‹µ ë©”íƒ€ë°ì´í„°"):
                st.json(
                    {
                        "í† í° ì‚¬ìš©ëŸ‰": response.additional_kwargs["usage"],
                        "ëª¨ë¸": response.additional_kwargs["model_id"],
                        "ì‘ë‹µ ID": response.id,
                    }
                )

except Exception as e:
    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    st.error("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. 'vector_db' ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
