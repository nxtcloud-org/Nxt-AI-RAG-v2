"""
RAG Retrieval Evaluator - Streamlit Application
ë‹¤ì¤‘ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ê²€ìƒ‰ í’ˆì§ˆì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¹„êµ í‰ê°€
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import os
import sys
import time
from dotenv import load_dotenv

load_dotenv()

# ==================== ì´ˆê¸° ì„¤ì • ====================
st.set_page_config(
    page_title="RAG Retrieval Evaluator",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# ëª¨ë“ˆ Import
try:
    import rag_retrieval_evaluator
    import importlib
    importlib.reload(rag_retrieval_evaluator)
    
    from rag_retrieval_evaluator import (
        ChromaDBRetriever, PostgreSQLRetriever, AWSKnowledgeBaseRetriever,
        RetrievalEvaluator, TEST_DATASET
    )
    from ragas.metrics import ContextPrecision, ContextRecall, Faithfulness, AnswerRelevancy
    import boto3
    from langchain_aws import ChatBedrock
except ImportError as e:
    st.error(f"ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    st.stop()

# ==================== ìŠ¤íƒ€ì¼ ====================
st.markdown("""
<style>
.retriever-card {
    background-color: #F0F2F6;
    border-radius: 12px;
    padding: 20px;
    margin-bottom: 20px;
    border: 1px solid #DDE1E6;
    transition: all 0.3s ease;
}
[data-theme="dark"] .retriever-card {
    background-color: #1E2129;
    border: 1px solid #30363D;
}
.retriever-card:hover {
    border-color: #58A6FF;
    box-shadow: 0 4px 12px rgba(88, 166, 255, 0.15);
}
.result-content {
    font-size: 1rem;
    line-height: 1.6;
    color: #1F2328;
    background: #FFFFFF;
    padding: 15px;
    border-radius: 8px;
    margin-top: 10px;
    border-left: 4px solid #0068C9;
}
[data-theme="dark"] .result-content {
    color: #C9D1D9;
    background: #0D1117;
    border-left-color: #58A6FF;
}
.rank-badge {
    background: #238636;
    color: white;
    padding: 2px 8px;
    border-radius: 4px;
    font-size: 0.8rem;
    font-weight: bold;
}
.system-header {
    background: #1E2129;
    padding: 10px;
    border-radius: 8px;
    border-bottom: 2px solid #58A6FF;
    margin-bottom: 20px;
}
.system-title {
    color: #58A6FF;
    font-weight: bold;
    text-align: center;
}
</style>
""", unsafe_allow_html=True)

# ==================== ì´ˆê¸°í™” í•¨ìˆ˜ ====================
@st.cache_resource
def initialize_retrievers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  Retriever ì´ˆê¸°í™”"""
    available = {}
    
    # ChromaDB
    pdf_path = os.getenv("CHROMA_PDF_PATH", os.path.abspath(os.path.join(current_dir, "../5_RAG/data/univ-data.pdf")))
    db_path = os.getenv("CHROMA_DB_PATH", os.path.abspath(os.path.join(current_dir, "../5_RAG/vector_db")))
    
    if os.path.exists(pdf_path):
        try:
            available["ChromaDB"] = ChromaDBRetriever(pdf_path=pdf_path, vector_db_path=db_path)
        except:
            pass

    # PostgreSQL
    if os.getenv("DB_HOST"):
        try:
            available["PostgreSQL"] = PostgreSQLRetriever()
        except:
            pass

    # AWS Knowledge Base
    kb_ids = [kb.strip() for kb in os.getenv("AWS_KB_IDS", "").split(",") if kb.strip()]
    if kb_ids:
        try:
            available["KnowledgeBase"] = AWSKnowledgeBaseRetriever(knowledge_base_ids=kb_ids)
        except:
            pass
        
    return available

@st.cache_resource
def initialize_llm():
    """ë‹µë³€ ìƒì„±ìš© LLM ì´ˆê¸°í™”"""
    try:
        client = boto3.client("bedrock-runtime", region_name="us-east-1")
        return ChatBedrock(
            client=client,
            model_id="anthropic.claude-3-haiku-20240307-v1:0",
            model_kwargs={"temperature": 0}
        )
    except:
        return None

def generate_answer(llm, question: str, contexts: list) -> str:
    """ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ìƒì„±"""
    if not contexts:
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    prompt = f"""ì°¸ê³  ìë£Œ:
{chr(10).join(contexts)}

ì§ˆë¬¸: {question}

ìœ„ ìë£Œì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. "ìë£Œì— ë”°ë¥´ë©´", "ì»¨í…ìŠ¤íŠ¸ì—ì„œ", "ìœ„ ë‚´ìš©ì„ ë³´ë©´" ë“±ì˜ ë©”íƒ€ í‘œí˜„ ê¸ˆì§€
2. "~ì…ë‹ˆë‹¤", "~í•©ë‹ˆë‹¤" ë“± ë‹¨ì •ì  ì–´ì¡° ì‚¬ìš©
3. í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬
4. ì§ˆë¬¸ì—ì„œ ë¬»ì§€ ì•Šì€ ì¶”ê°€ ì„¤ëª… ê¸ˆì§€

ë‹µë³€:"""

    try:
        return llm.invoke(prompt).content
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# ==================== ì „ì—­ ë³€ìˆ˜ ====================
all_retrievers = initialize_retrievers()
llm = initialize_llm()

METRICS = {
    "context_precision": {"name": "Context Precision (ë¬¸ë§¥ ì •ë°€ë„)", "instance": ContextPrecision(), "default": True},
    "context_recall": {"name": "Context Recall (ë¬¸ë§¥ ì¬í˜„ìœ¨)", "instance": ContextRecall(), "default": True},
    "faithfulness": {"name": "Faithfulness (ì¶©ì‹¤ë„)", "instance": Faithfulness(), "default": False},
    "answer_relevancy": {"name": "Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±)", "instance": AnswerRelevancy(), "default": False}
}

# ==================== ì‚¬ì´ë“œë°” ====================
st.sidebar.title("ğŸš€ RAG Controller")

st.sidebar.markdown("### ğŸ” Retriever ì„ íƒ")
selected_retrievers = [name for name in all_retrievers if st.sidebar.checkbox(name, value=True)]

st.sidebar.markdown("### ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ì„ íƒ")
selected_metrics = [info["instance"] for key, info in METRICS.items() if st.sidebar.checkbox(info["name"], value=info["default"])]

if not selected_metrics:
    st.sidebar.warning("âš ï¸ ìµœì†Œ í•˜ë‚˜ì˜ ë©”íŠ¸ë¦­ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

# ==================== ë©”ì¸ UI ====================
st.title("ğŸ” Multi-Vector RAG Evaluator")
st.markdown("ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—”ë“œì˜ ê²€ìƒ‰ í’ˆì§ˆì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.")

with st.expander("ğŸ“… í‰ê°€ ë°ì´í„°ì…‹ ì •ë³´"):
    st.write(f"í˜„ì¬ ë¡œë“œëœ ì§ˆë¬¸ ìˆ˜: **{len(TEST_DATASET['questions'])}**ê°œ")
    st.markdown("### ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸")
    for i, q in enumerate(TEST_DATASET['questions'], 1):
        st.text(f"{i}. {q}")

# ìœ íš¨ì„± ê²€ì‚¬
if not selected_retrievers:
    st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ìµœì†Œ í•˜ë‚˜ì˜ Retrieverë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    st.stop()

if not selected_metrics:
    st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ìµœì†Œ í•˜ë‚˜ì˜ í‰ê°€ ë©”íŠ¸ë¦­ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    st.stop()

# ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸
if 'mode' not in st.session_state:
    st.session_state.mode = 'single'

# ëª¨ë“œ ì„ íƒ
st.markdown("---")
col1, col2 = st.columns(2)
if col1.button("ğŸ§ª ë‹¨ì¼ í…ŒìŠ¤íŠ¸", type="primary", use_container_width=True):
    st.session_state.mode = 'single'
if col2.button("ğŸ“ˆ ë°°ì¹˜ í‰ê°€", use_container_width=True):
    st.session_state.mode = 'batch'

# ==================== ë‹¨ì¼ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ====================
def display_single_results(query, k, retriever_names):
    """ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ (ë³‘ë ¬ ì²˜ë¦¬)"""
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    st.markdown("### ğŸ¯ ê²€ìƒ‰ ê²°ê³¼ ë¹„êµ")
    
    # ë³‘ë ¬ë¡œ ê²€ìƒ‰ ì‹¤í–‰
    def retrieve_with_timing(name):
        start = time.time()
        try:
            results = all_retrievers[name].retrieve(query, k=k)
            duration = time.time() - start
            return name, results, duration, None
        except Exception as e:
            duration = time.time() - start
            return name, None, duration, str(e)
    
    # ë³‘ë ¬ ì‹¤í–‰
    search_results = {}
    with ThreadPoolExecutor(max_workers=len(retriever_names)) as executor:
        futures = {executor.submit(retrieve_with_timing, name): name for name in retriever_names}
        
        for future in as_completed(futures):
            name, results, duration, error = future.result()
            search_results[name] = {
                'results': results,
                'duration': duration,
                'error': error
            }
    
    # ê²°ê³¼ í‘œì‹œ (ì›ë˜ ìˆœì„œëŒ€ë¡œ)
    cols = st.columns(len(retriever_names))
    
    for idx, name in enumerate(retriever_names):
        with cols[idx]:
            st.markdown(f"<div class='system-header'><div class='system-title'>{name}</div></div>", unsafe_allow_html=True)
            
            data = search_results[name]
            st.caption(f"â±ï¸ DB ì¡°íšŒ: {data['duration']:.3f}ì´ˆ")
            
            if data['error']:
                st.error(f"ì˜¤ë¥˜: {data['error']}")
            elif not data['results']:
                st.info("ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                results = data['results']
                
                # AI ë‹µë³€ ìƒì„± (ì‹œê°„ ì¸¡ì • ì œì™¸)
                if llm:
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                        answer = generate_answer(llm, query, results)
                        st.markdown("**ğŸ¤– AI ìƒì„± ë‹µë³€**")
                        st.info(answer)
                    st.markdown("---")
                
                for rank, result in enumerate(results, 1):
                    st.markdown(f"""
                    <div class='retriever-card'>
                        <span class='rank-badge'>ìˆœìœ„ {rank}</span>
                        <div class='result-content'>{result}</div>
                    </div>
                    """, unsafe_allow_html=True)

# ==================== ë°°ì¹˜ í‰ê°€ í•¨ìˆ˜ ====================
def run_batch_evaluation(retrievers, metrics, k_value):
    """ë°°ì¹˜ í‰ê°€ ì‹¤í–‰"""
    st.markdown("---")
    st.subheader("ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„")

    retriever_list = [all_retrievers[name] for name in retrievers]
    evaluator = RetrievalEvaluator(
        retrievers=retriever_list,
        questions=TEST_DATASET["questions"],
        gold_contexts=TEST_DATASET["gold_contexts"]
    )

    completed = st.container()
    
    with st.status("í‰ê°€ ì¤‘...", expanded=True) as status:
        # 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰
        st.write("ğŸ“‹ **1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰**")
        all_results = {}
        for name in retrievers:
            st.write(f"   â†’ {name} ê²€ìƒ‰ ì¤‘...")
            results = [all_retrievers[name].retrieve(q, k=k_value) for q in TEST_DATASET["questions"]]
            all_results[all_retrievers[name].get_system_name()] = results
        
        with completed:
            st.success("âœ… **1ë‹¨ê³„ ì™„ë£Œ**: ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
        
        # 1.5ë‹¨ê³„: ë‹µë³€ ìƒì„±
        has_gen = any(isinstance(m, (Faithfulness, AnswerRelevancy)) for m in metrics)
        all_answers = None
        
        if has_gen:
            if llm is None:
                st.warning("âš ï¸ LLM ì´ˆê¸°í™” ì‹¤íŒ¨. ê²€ìƒ‰ ë©”íŠ¸ë¦­ë§Œ í‰ê°€í•©ë‹ˆë‹¤.")
                metrics = [m for m in metrics if not isinstance(m, (Faithfulness, AnswerRelevancy))]
            else:
                st.write("ğŸ¤– **1.5ë‹¨ê³„: ë‹µë³€ ìƒì„±**")
                progress_bar = st.progress(0)
                progress_text = st.empty()
                
                total = len(retrievers) * len(TEST_DATASET["questions"])
                current = 0
                
                all_answers = {}
                for system_name, contexts_list in all_results.items():
                    system_answers = []
                    for q, ctx in zip(TEST_DATASET["questions"], contexts_list):
                        system_answers.append(generate_answer(llm, q, ctx))
                        current += 1
                        progress = current / total
                        progress_bar.progress(progress)
                        progress_text.text(f"ë‹µë³€ ìƒì„± ì¤‘... {current}/{total} ({progress*100:.1f}%)")
                    all_answers[system_name] = system_answers
                
                progress_bar.empty()
                progress_text.empty()
                
                with completed:
                    st.success("âœ… **1.5ë‹¨ê³„ ì™„ë£Œ**: ë‹µë³€ ìƒì„± ì™„ë£Œ")
        
        # 2ë‹¨ê³„: ë©”íŠ¸ë¦­ í‰ê°€
        st.write("ğŸ“Š **2ë‹¨ê³„: ë©”íŠ¸ë¦­ í‰ê°€**")
        eval_progress = st.progress(0)
        eval_text = st.empty()
        
        all_dfs = {}
        total_sys = len(all_results)
        
        for idx, (system_name, contexts) in enumerate(all_results.items()):
            eval_text.text(f"í‰ê°€ ì¤‘... {system_name} ({idx+1}/{total_sys})")
            system_answers = all_answers.get(system_name) if all_answers else None
            df = evaluator.evaluate_system(system_name, contexts, system_answers)
            
            if df is not None:
                all_dfs[system_name] = df
            
            eval_progress.progress((idx + 1) / total_sys)
        
        eval_progress.empty()
        eval_text.empty()
        
        with completed:
            st.success("âœ… **2ë‹¨ê³„ ì™„ë£Œ**: ë©”íŠ¸ë¦­ í‰ê°€ ì™„ë£Œ")
        
        comparison_df = evaluator.create_comparison_report(all_dfs)
        status.update(label="âœ… í‰ê°€ ì™„ë£Œ!", state="complete", expanded=False)
    
    # ì‹œê°í™”
    display_visualizations(comparison_df, all_dfs)
    display_detailed_logs(all_dfs)
    
    st.balloons()

def display_visualizations(comparison_df, all_dfs):
    """ì‹œê°í™” í‘œì‹œ"""
    if comparison_df is None or comparison_df.empty:
        st.error("âš ï¸ ë¹„êµ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if not all(col in comparison_df.columns for col in ['Metric', 'Mean', 'System']):
        st.warning("âš ï¸ ë¹„êµ ë¦¬í¬íŠ¸ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
        st.dataframe(comparison_df)
        return
    
    st.markdown("---")
    st.markdown("## ğŸ“Š ì‹œê°í™”")
    
    # ì „ì²´ í†µê³„ ìš”ì•½
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("í‰ê°€ ì‹œìŠ¤í…œ ìˆ˜", len(all_dfs))
    with col2:
        st.metric("í‰ê°€ ë©”íŠ¸ë¦­ ìˆ˜", len(comparison_df['Metric'].unique()))
    with col3:
        avg_score = comparison_df['Mean'].mean()
        st.metric("ì „ì²´ í‰ê·  ì ìˆ˜", f"{avg_score:.3f}")
    with col4:
        best_system = comparison_df.groupby('System')['Mean'].mean().idxmax()
        st.metric("ìµœê³  ì„±ëŠ¥", best_system)
    
    st.markdown("---")
    
    # ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë¹„êµ
    st.markdown("### ğŸ“Š ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë¹„êµ")
    fig_bar = px.bar(
        comparison_df, x="Metric", y="Mean", color="System",
        barmode="group", title="ë°ì´í„°ë² ì´ìŠ¤ë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥ ë¹„êµ",
        template="plotly_dark", height=400
    )
    st.plotly_chart(fig_bar, use_container_width=True)
    
    st.markdown("---")
    
    # ì„±ëŠ¥ íˆíŠ¸ë§µ
    st.markdown("### ğŸ“Š ì„±ëŠ¥ íˆíŠ¸ë§µ")
    pivot = comparison_df.pivot(index="System", columns="Metric", values="Mean")
    
    fig_heat = go.Figure(data=go.Heatmap(
        z=pivot.values,
        x=pivot.columns.tolist(),
        y=pivot.index.tolist(),
        colorscale='Blues',
        text=pivot.values,
        texttemplate='%{text:.3f}',
        textfont={"size": 12},
        colorbar=dict(title="Score")
    ))
    
    fig_heat.update_layout(xaxis_title="Metric", yaxis_title="System", height=300)
    st.plotly_chart(fig_heat, use_container_width=True)
    
    st.markdown("---")
    
    # ì‹œìŠ¤í…œë³„ ì¢…í•© ì ìˆ˜
    st.markdown("### ğŸ† ì‹œìŠ¤í…œë³„ ì¢…í•© ì ìˆ˜")
    system_avg = comparison_df.groupby('System')['Mean'].mean().sort_values(ascending=False)
    
    cols = st.columns(len(system_avg))
    for idx, ((system, score), col) in enumerate(zip(system_avg.items(), cols), 1):
        medal = "ğŸ¥‡" if idx == 1 else "ğŸ¥ˆ" if idx == 2 else "ğŸ¥‰" if idx == 3 else "ğŸ“Š"
        with col:
            st.metric(f"{medal} {idx}ìœ„", system, f"{score:.3f}")

def display_detailed_logs(all_dfs):
    """ìƒì„¸ í‰ê°€ ë¡œê·¸ í‘œì‹œ"""
    if not all_dfs:
        return
    
    st.markdown("---")
    st.markdown("### ğŸ“„ ìƒì„¸ í‰ê°€ ë¡œê·¸")
    
    tabs = st.tabs(list(all_dfs.keys()))
    
    for idx, system_name in enumerate(all_dfs.keys()):
        with tabs[idx]:
            df = all_dfs[system_name]
            numeric_cols = df.select_dtypes(include=['number']).columns
            
            # ì‹œìŠ¤í…œë³„ ìš”ì•½
            st.markdown(f"#### {system_name} í‰ê°€ ìš”ì•½")
            col_sum1, col_sum2 = st.columns(2)
            
            with col_sum1:
                st.markdown("**í‰ê·  Â± í‘œì¤€í¸ì°¨**")
                for col in numeric_cols:
                    avg, std = df[col].mean(), df[col].std()
                    st.write(f"â€¢ **{col}**: {avg:.3f} (Â±{std:.3f})")
            
            with col_sum2:
                st.markdown("**ìµœì†Œ ~ ìµœëŒ€**")
                for col in numeric_cols:
                    min_val, max_val = df[col].min(), df[col].max()
                    st.write(f"â€¢ **{col}**: {min_val:.3f} ~ {max_val:.3f}")
            
            st.markdown("---")
            st.markdown("**ì „ì²´ í‰ê°€ ë°ì´í„°**")
            
            # ì¹´ë“œ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
            for row_idx in range(len(df)):
                display_evaluation_card(df.iloc[row_idx], row_idx, numeric_cols)

def display_evaluation_card(row, row_idx, numeric_cols):
    """ê°œë³„ í‰ê°€ ì¹´ë“œ í‘œì‹œ"""
    # ì ìˆ˜ ê³„ì‚°
    scores = [row[col] for col in numeric_cols if pd.notna(row[col])]
    avg_score = sum(scores) / len(scores) if scores else 0
    
    # ì ìˆ˜ì— ë”°ë¥¸ ìƒ‰ìƒ
    if avg_score >= 0.9:
        score_color = "ğŸŸ¢"
    elif avg_score >= 0.7:
        score_color = "ğŸŸ¡"
    else:
        score_color = "ğŸ”´"
    
    # ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°
    question_preview = ""
    if 'user_input' in row:
        question_text = str(row['user_input'])
        question_preview = question_text[:80] + "..." if len(question_text) > 80 else question_text
    
    # Expander ì œëª©
    expander_title = f"{score_color} ì§ˆë¬¸ {row_idx + 1}"
    if question_preview:
        expander_title += f" {question_preview}"
    expander_title += f" - í‰ê·  ì ìˆ˜: {avg_score:.3f}"
    
    with st.expander(expander_title):
        # ì§ˆë¬¸
        if 'user_input' in row:
            st.markdown("**ğŸ“ ì§ˆë¬¸**")
            st.info(row['user_input'])
        
        # ë‹µë³€
        if 'response' in row:
            st.markdown("**ğŸ¤– AI ë‹µë³€**")
            st.success(row['response'])
        
        # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
        if 'retrieved_contexts' in row:
            st.markdown("**ğŸ“š ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸**")
            contexts = row['retrieved_contexts']
            if isinstance(contexts, list):
                for ctx_idx, ctx in enumerate(contexts, 1):
                    st.markdown(f"**{ctx_idx}.** {ctx}")
            else:
                st.markdown(contexts)
        
        # ì •ë‹µ ì°¸ì¡°
        if 'reference' in row:
            st.markdown("**âœ… ì •ë‹µ ì°¸ì¡°**")
            st.warning(row['reference'])
        
        st.markdown("---")
        
        # ë©”íŠ¸ë¦­ ì ìˆ˜
        st.markdown("**ğŸ“Š ë©”íŠ¸ë¦­ ì ìˆ˜**")
        metric_cols = st.columns(len(numeric_cols))
        for col_idx, col in enumerate(numeric_cols):
            with metric_cols[col_idx]:
                score = row[col]
                if pd.notna(score):
                    st.metric(col, f"{score:.3f}")
                else:
                    st.metric(col, "N/A")

# ==================== ëª¨ë“œë³„ ì‹¤í–‰ ====================
if st.session_state.mode == 'single':
    st.markdown("---")
    st.subheader("ğŸ§ª ë‹¨ì¼ í…ŒìŠ¤íŠ¸")
    
    col_input, col_k = st.columns([4, 1])
    query = col_input.text_input("ê²€ìƒ‰ ì¿¼ë¦¬:", placeholder="ì˜ˆ: ì¡°ê¸°ì¡¸ì—… ìš”ê±´ì´ ë­ì•¼?")
    k = col_k.number_input("Top K", min_value=1, max_value=10, value=3)
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        search_clicked = st.button("ğŸ” ê²€ìƒ‰ ì‹¤í–‰", type="primary", use_container_width=True)
    
    if search_clicked:
        if query:
            display_single_results(query, k, selected_retrievers)
        else:
            st.warning("ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

else:  # batch mode
    st.markdown("---")
    st.subheader("ğŸ“ˆ ë°°ì¹˜ í‰ê°€")
    
    col_left, col_right = st.columns([3, 1])
    
    with col_left:
        st.info(f"""
        **í‰ê°€ ì„¤ì •:**
        - Retriever: {', '.join(selected_retrievers)}
        - ì§ˆë¬¸ ìˆ˜: {len(TEST_DATASET['questions'])}ê°œ
        - ë©”íŠ¸ë¦­: {len(selected_metrics)}ê°œ
        """)
    
    with col_right:
        batch_k = st.number_input("Top K", min_value=1, max_value=10, value=3, key="batch_k")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        eval_clicked = st.button("ğŸš€ í‰ê°€ ì‹œì‘", type="primary", use_container_width=True)
    
    if eval_clicked:
        run_batch_evaluation(selected_retrievers, selected_metrics, batch_k)